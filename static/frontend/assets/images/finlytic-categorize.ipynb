{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMrWojiFTZ/OLE1Au6RXXcr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers datasets tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"odjxyDJxbbQx","executionInfo":{"status":"ok","timestamp":1727491731791,"user_tz":-345,"elapsed":27581,"user":{"displayName":"Basab Jha","userId":"01698150105745770629"}},"outputId":"a329f296-e93b-43d0-f894-45b312433faf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Collecting datasets\n","  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIredooBbYwr"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, GPT2Config\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","import os\n","import logging\n","import numpy as np\n","from tqdm import tqdm\n","import random\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Part 1: Data Generation and Preparation\n","\n","def generate_sme_expense_data(file_name, num_samples=500):\n","    categories = [\n","        'Retail', 'Manufacturing', 'IT Services', 'Consultancy',\n","        'Hospitality', 'Construction', 'Agriculture', 'Transport',\n","        'Healthcare', 'Education'\n","    ]\n","\n","    descriptions = [\n","        \"Monthly rent payment\", \"Purchase of raw materials\",\n","        \"Software license renewal\", \"Employee salaries\",\n","        \"Utility bills\", \"Advertising costs\",\n","        \"Equipment maintenance\", \"Staff training expenses\",\n","        \"Insurance premium\", \"Office supplies purchase\",\n","        \"Travel expenses\", \"Professional fees\",\n","        \"Inventory restocking\", \"Vehicle fuel costs\",\n","        \"Loan interest payment\", \"Depreciation of assets\"\n","    ]\n","\n","    tax_categories = [\n","        'Deductible Expense', 'Depreciable Asset', 'Non-Deductible Expense',\n","        'Partially Deductible', 'VAT Applicable', 'Capital Expenditure', 'Other'\n","    ]\n","\n","    data = {\n","        'Business_Type': [],\n","        'Description': [],\n","        'Amount': [],\n","        'Tax_Category': []\n","    }\n","\n","    for _ in range(num_samples):\n","        data['Business_Type'].append(random.choice(categories))\n","        data['Description'].append(random.choice(descriptions))\n","        data['Amount'].append(random.randint(1000, 500000))  # Amount in NPR\n","        data['Tax_Category'].append(random.choice(tax_categories))\n","\n","    df = pd.DataFrame(data)\n","    df.to_excel(file_name, index=False)\n","    logging.info(f\"Generated {num_samples} SME expense records and saved to {file_name}\")\n","\n","    return df\n","\n","# Generate training and test data\n","train_df = generate_sme_expense_data('sme_expenses_train.xlsx', num_samples=2000)\n","test_df = generate_sme_expense_data('sme_expenses_test.xlsx', num_samples=500)\n","\n","# Part 2: Model Training\n","\n","model_name = \"gpt2-medium\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","config = GPT2Config.from_pretrained(model_name)\n","config.num_labels = 1  # For regression task\n","model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"left\"\n","\n","def preprocess_function(examples):\n","    full_text = [f\"<|startoftext|>Business Type: {b}\\nExpense: {e}\\nAmount: {a} NPR\\nTax Category: {t}<|endoftext|>\"\n","                 for b, e, a, t in zip(examples['Business_Type'], examples['Description'],\n","                                       examples['Amount'], examples['Tax_Category'])]\n","\n","    tokenized = tokenizer(full_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n","    tokenized['labels'] = tokenized['input_ids'].clone()\n","\n","    for key in tokenized.keys():\n","        tokenized[key] = tokenized[key].tolist()\n","\n","    return tokenized\n","\n","train_dataset = Dataset.from_pandas(train_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n","tokenized_test = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./finlytic_categorize_results\",\n","    per_device_train_batch_size=32,  # Increase batch size\n","    per_device_eval_batch_size=64,\n","    num_train_epochs=3,  # Reduce number of epochs\n","    logging_dir=\"./finlytic_categorize_logs\",\n","    logging_steps=10,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=100,\n","    save_steps=100,\n","    warmup_steps=500,\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n","    fp16=True,\n","    load_best_model_at_end=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_test,\n",")\n","\n","logging.info(\"Starting model training...\")\n","trainer.train()\n","logging.info(\"Model training completed.\")\n","\n","model_save_path = \"./finlytic_categorize_model\"\n","model.save_pretrained(model_save_path)\n","tokenizer.save_pretrained(model_save_path)\n","logging.info(f\"Model saved to {model_save_path}\")\n","\n","# Part 3: Expense Categorization and Advice Generation\n","\n","def categorize_expense(business_type, description, amount, model, tokenizer):\n","    input_text = f\"<|startoftext|>Business Type: {business_type}\\nExpense: {description}\\nAmount: {amount} NPR\\nTax Category:\"\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","\n","    with torch.no_grad():\n","        output = model.generate(input_ids, max_length=150, num_return_sequences=1,\n","                                temperature=0.7, top_k=50, top_p=0.95, do_sample=True)\n","\n","    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","    tax_category = generated_text.split(\"Tax Category:\")[-1].strip()\n","\n","    return tax_category\n","\n","def generate_tax_advice(business_type, description, amount, tax_category):\n","    advice = f\"For the expense '{description}' of NPR {amount} in your {business_type} business:\\n\\n\"\n","\n","    if tax_category == \"Deductible Expense\":\n","        advice += \"This expense is fully deductible. Keep proper documentation for your tax records.\"\n","    elif tax_category == \"Depreciable Asset\":\n","        advice += \"This is a depreciable asset. You can claim depreciation over its useful life.\"\n","    elif tax_category == \"Non-Deductible Expense\":\n","        advice += \"This expense is not deductible for tax purposes. Consider if it's necessary for your business.\"\n","    elif tax_category == \"Partially Deductible\":\n","        advice += \"This expense is partially deductible. Consult with a tax professional to determine the deductible portion.\"\n","    elif tax_category == \"VAT Applicable\":\n","        advice += \"VAT is applicable on this expense. Ensure you have a valid VAT invoice to claim input tax credit.\"\n","    elif tax_category == \"Capital Expenditure\":\n","        advice += \"This is a capital expenditure. It should be capitalized and depreciated over time rather than expensed immediately.\"\n","    else:\n","        advice += \"The tax treatment of this expense is not straightforward. Consult with a tax professional for specific advice.\"\n","\n","    return advice\n","\n","# Load the fine-tuned model and tokenizer\n","logging.info(\"Loading fine-tuned model and tokenizer...\")\n","model_path = \"./finlytic_categorize_model\"\n","model = GPT2LMHeadModel.from_pretrained(model_path)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","logging.info(\"Model and tokenizer loaded successfully.\")\n","\n","def get_expense_advice(business_type, description, amount):\n","    tax_category = categorize_expense(business_type, description, amount, model, tokenizer)\n","    advice = generate_tax_advice(business_type, description, amount, tax_category)\n","    return advice\n","\n","# Function to process an input Excel file\n","def process_expense_file(input_file, output_file):\n","    logging.info(f\"Processing input file: {input_file}\")\n","    df = pd.read_excel(input_file)\n","\n","    results = []\n","    for _, row in df.iterrows():\n","        advice = get_expense_advice(row['Business_Type'], row['Description'], row['Amount'])\n","        results.append({\n","            'Business_Type': row['Business_Type'],\n","            'Description': row['Description'],\n","            'Amount': row['Amount'],\n","            'Tax_Advice': advice\n","        })\n","\n","    output_df = pd.DataFrame(results)\n","    output_df.to_excel(output_file, index=False)\n","    logging.info(f\"Results saved to {output_file}\")\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    input_file = \"input_expenses.xlsx\"  # Replace with your input file name\n","    output_file = \"expense_advice_output.xlsx\"\n","\n","    if not os.path.exists(input_file):\n","        logging.error(f\"Input file {input_file} not found.\")\n","    else:\n","        process_expense_file(input_file, output_file)\n","        logging.info(\"Expense categorization and advice generation completed.\")"]}]}